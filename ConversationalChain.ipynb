{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#AIM: To build Memory based chat system\n",
        "#create an object where i can store my messages\n",
        "#update this object with user message and AI reponse (write a function which can update our memory object)\n",
        "#create a chain (pt, model and parser)\n",
        "#In order to pass history to my chain, i need to make my PT accordingly which can accept history and new message\n",
        "#whenever i call my chain, i should also invoke my memory method: so lets make a method which include chain invokation and memory call\n"
      ],
      "metadata": {
        "id": "QRJRuJfo77qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_core langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34uVv7OIqzbM",
        "outputId": "05886104-b410-4e68-8096-5499fa908e6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/411.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zHvLD8jPjCpY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize memory (list of messages)\n",
        "memory = []\n",
        "\n",
        "# Define the prompt template with the variables to replace\n",
        "template = \"\"\"\n",
        "You are a helpful assistant. Here is the conversation history so far:\n",
        "{history}\n",
        "\n",
        "Now, respond to the following question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY']=userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "buamgdhirKnC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to update memory with each new message\n",
        "def update_memory(memory, question, response):\n",
        "    memory.append(f\"User: {question}\")\n",
        "    memory.append(f\"Assistant: {response}\")\n",
        "    return memory"
      ],
      "metadata": {
        "id": "8eDvbsMyjG03"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq()\n",
        "output_parser = StrOutputParser()\n",
        "conversation_chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "4KfWYauw72gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question, memory):\n",
        "\n",
        "    # stringify the history of messages\n",
        "    history = \"\\n\".join(memory)\n",
        "\n",
        "    # pass the history and the question to the chain\n",
        "    result = conversation_chain.invoke({\n",
        "        \"history\": history,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    memory = update_memory(memory, question, result)\n",
        "\n",
        "    return result, memory\n"
      ],
      "metadata": {
        "id": "Ydp1mi_ijLLC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the conversation\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    response, memory  = get_response(user_input, memory)\n",
        "    print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "6LbO0HYojNY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}